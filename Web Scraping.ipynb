{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000 reviews\n",
      "Collected 1010 star ratings\n",
      "Collected 1000 dates\n",
      "Collected 1000 countries\n"
     ]
    }
   ],
   "source": [
    "# Base URL\n",
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "\n",
    "# Number of pages to scrape\n",
    "pages = 10  # As per your range\n",
    "\n",
    "# Create empty lists to collect data\n",
    "reviews = []\n",
    "stars = []\n",
    "date = []\n",
    "country = []\n",
    "\n",
    "# Loop through each page\n",
    "for i in range(1, pages + 1):\n",
    "    # Make a GET request to fetch the raw HTML content\n",
    "    page = requests.get(f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize=100\")\n",
    "    \n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Extract reviews\n",
    "    for item in soup.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(item.get_text())\n",
    "    \n",
    "    # Extract stars\n",
    "    for item in soup.find_all(\"div\", class_=\"rating-10\"):\n",
    "        try:\n",
    "            stars.append(item.span.text.strip())\n",
    "        except AttributeError:\n",
    "            print(f\"Error on page {i} for stars\")\n",
    "            stars.append(\"None\")\n",
    "            \n",
    "    # Extract date\n",
    "    for item in soup.find_all(\"time\"):\n",
    "        date.append(item.text.strip())\n",
    "        \n",
    "    # Extract country\n",
    "    for item in soup.find_all(\"h3\"):\n",
    "        try:\n",
    "            country.append(item.span.next_sibling.text.strip(\" ()\"))\n",
    "        except AttributeError:\n",
    "            print(f\"Error on page {i} for country\")\n",
    "            country.append(\"None\")\n",
    "\n",
    "# Print the length of the collected lists to verify\n",
    "print(f\"Collected {len(reviews)} reviews\")\n",
    "print(f\"Collected {len(stars)} star ratings\")\n",
    "print(f\"Collected {len(date)} dates\")\n",
    "print(f\"Collected {len(country)} countries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>✅ Trip Verified |  My family flew from Washing...</td>\n",
       "      <td>1</td>\n",
       "      <td>19th August 2023</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review Stars  \\\n",
       "190  ✅ Trip Verified |  My family flew from Washing...     1   \n",
       "\n",
       "                 Date        Country  \n",
       "190  19th August 2023  United States  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure lengths are synchronized\n",
    "min_length = min(len(reviews), len(stars), len(date), len(country))\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame({\n",
    "    'Review': reviews[:min_length],\n",
    "    'Stars': stars[:min_length],\n",
    "    'Date': date[:min_length],\n",
    "    'Country': country[:min_length]\n",
    "})\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>✅ Verified Review |  London Heathrow to Paris ...</td>\n",
       "      <td>4</td>\n",
       "      <td>17th August 2017</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>✅ Verified Review |  Bari to Gatwick. More of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>23rd October 2017</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>CPH-LHR-CPH October 2014. Air travel just keep...</td>\n",
       "      <td>9</td>\n",
       "      <td>25th November 2014</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>Not Verified |  \\r\\nMiami to London Heathrow w...</td>\n",
       "      <td>2</td>\n",
       "      <td>30th April 2019</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>I usually have very positive experiences when ...</td>\n",
       "      <td>7</td>\n",
       "      <td>6th April 2015</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>✅ Verified Review |  Flew London Heathrow to L...</td>\n",
       "      <td>3</td>\n",
       "      <td>5th October 2016</td>\n",
       "      <td>Portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>✅ Trip Verified | I had booked business class ...</td>\n",
       "      <td>4</td>\n",
       "      <td>5th February 2018</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>✅ Trip Verified | The check-in process was smo...</td>\n",
       "      <td>10</td>\n",
       "      <td>25th September 2022</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>✅ Trip Verified |  My family and I have flown ...</td>\n",
       "      <td>9</td>\n",
       "      <td>9th July 2023</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>✅ Verified Review |  Istanbul to London Heathr...</td>\n",
       "      <td>5</td>\n",
       "      <td>23rd February 2017</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>The British Airways plane I flew on from Larna...</td>\n",
       "      <td>9</td>\n",
       "      <td>27th August 2015</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Not Verified |  Booked a very special holiday ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5th October 2023</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>LHR-LAX-LHR Economy. Both legs on lower deck o...</td>\n",
       "      <td>4</td>\n",
       "      <td>11th February 2015</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>✅ Trip Verified |  Phoenix to Accra via London...</td>\n",
       "      <td>6</td>\n",
       "      <td>30th November 2018</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>✅ Trip Verified | London Heathrow to Seychelle...</td>\n",
       "      <td>10</td>\n",
       "      <td>20th October 2019</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>✅ Verified Review |  This flight from Copenhag...</td>\n",
       "      <td>10</td>\n",
       "      <td>4th April 2016</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>✅ Trip Verified |  Prague to London. A two hou...</td>\n",
       "      <td>9</td>\n",
       "      <td>27th July 2019</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Not Verified |  This flight failed at every le...</td>\n",
       "      <td>1</td>\n",
       "      <td>20th June 2023</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Not Verified |  I’ve generally been a loyal Go...</td>\n",
       "      <td>1</td>\n",
       "      <td>30th October 2022</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>IST-LHR the so called business class is a joke...</td>\n",
       "      <td>10</td>\n",
       "      <td>8th May 2015</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Stars  \\\n",
       "1618  ✅ Verified Review |  London Heathrow to Paris ...     4   \n",
       "1538  ✅ Verified Review |  Bari to Gatwick. More of ...     2   \n",
       "3389  CPH-LHR-CPH October 2014. Air travel just keep...     9   \n",
       "1018  Not Verified |  \\r\\nMiami to London Heathrow w...     2   \n",
       "3175  I usually have very positive experiences when ...     7   \n",
       "2184  ✅ Verified Review |  Flew London Heathrow to L...     3   \n",
       "1417  ✅ Trip Verified | I had booked business class ...     4   \n",
       "421   ✅ Trip Verified | The check-in process was smo...    10   \n",
       "217   ✅ Trip Verified |  My family and I have flown ...     9   \n",
       "1921  ✅ Verified Review |  Istanbul to London Heathr...     5   \n",
       "2880  The British Airways plane I flew on from Larna...     9   \n",
       "148   Not Verified |  Booked a very special holiday ...     1   \n",
       "3249  LHR-LAX-LHR Economy. Both legs on lower deck o...     4   \n",
       "1153  ✅ Trip Verified |  Phoenix to Accra via London...     6   \n",
       "841   ✅ Trip Verified | London Heathrow to Seychelle...    10   \n",
       "2446  ✅ Verified Review |  This flight from Copenhag...    10   \n",
       "937   ✅ Trip Verified |  Prague to London. A two hou...     9   \n",
       "239   Not Verified |  This flight failed at every le...     1   \n",
       "403   Not Verified |  I’ve generally been a loyal Go...     1   \n",
       "3119  IST-LHR the so called business class is a joke...    10   \n",
       "\n",
       "                     Date               Country  \n",
       "1618     17th August 2017        United Kingdom  \n",
       "1538    23rd October 2017        United Kingdom  \n",
       "3389   25th November 2014               Denmark  \n",
       "1018      30th April 2019        United Kingdom  \n",
       "3175       6th April 2015        United Kingdom  \n",
       "2184     5th October 2016              Portugal  \n",
       "1417    5th February 2018        United Kingdom  \n",
       "421   25th September 2022         United States  \n",
       "217         9th July 2023        United Kingdom  \n",
       "1921   23rd February 2017                Canada  \n",
       "2880     27th August 2015        United Kingdom  \n",
       "148      5th October 2023        United Kingdom  \n",
       "3249   11th February 2015               Germany  \n",
       "1153   30th November 2018                 Ghana  \n",
       "841     20th October 2019        United Kingdom  \n",
       "2446       4th April 2016               Denmark  \n",
       "937        27th July 2019        United Kingdom  \n",
       "239        20th June 2023        United Kingdom  \n",
       "403     30th October 2022        United Kingdom  \n",
       "3119         8th May 2015  United Arab Emirates  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
